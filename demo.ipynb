{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비디오 로드\n",
    "# 비디오 피처 뽑기\n",
    "# 오디오 -> spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from functools import partial\n",
    "from timm.models.vision_transformer import DropPath, Mlp, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSRVTT_DataLoader(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path,\n",
    "            we, #word embewdding\n",
    "            we_dim=300,\n",
    "            max_words=30,\n",
    "            num_frames_multiplier=5, #오디에 데이터 길이 조절용\n",
    "            training=True\n",
    "    ):\n",
    "        self.data = pickle.load(open(data_path, 'rb')) #pkl파일을 바이트 스트림(이진 모드)\n",
    "        self.we = we\n",
    "        self.we_dim = we_dim\n",
    "        self.max_words = max_words\n",
    "        self.max_video = 30\n",
    "        self.num_frames_multiplier = num_frames_multiplier\n",
    "        self.training = training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _zero_pad_tensor(self, tensor, size):  #입력 텐서의 크기를 고정된 크기로 만들기\n",
    "        if len(tensor) >= size:\n",
    "            return tensor[:size]\n",
    "        else:\n",
    "            zero = np.zeros((size - len(tensor), self.we_dim), dtype=np.float32)\n",
    "            return np.concatenate((tensor, zero), axis=0)\n",
    "\n",
    "    def _tokenize_text(self, sentence):  #텍스트를 단어 또는 부분 문자열로 분할\n",
    "        w = re.findall(r\"[\\w']+\", str(sentence))\n",
    "        return w\n",
    "\n",
    "    def _words_to_we(self, words):  #단어를 임베딩 벡터로 변환\n",
    "        # words = [word for word in words if word in self.we.vocab]\n",
    "        words = [word for word in words if word in self.we.key_to_index]\n",
    "        if words: #해당 단어가 임베딩 모델에 존재할 때 벡터 추출(학습 한 것만)\n",
    "            we = self._zero_pad_tensor(self.we[words], self.max_words)\n",
    "            return torch.from_numpy(we)\n",
    "        else:\n",
    "            return torch.zeros(self.max_words, self.we_dim)\n",
    "\n",
    "    def _get_caption(self, idx):\n",
    "        \"\"\"Chooses random caption if training. Uses set caption if evaluating.\"\"\"\n",
    "        if self.training: #훈련중일 경우 무작위 caption 가져오기\n",
    "            captions = self.data[idx]['caption']\n",
    "            caption = self._words_to_we(self._tokenize_text(random.choice(captions)))\n",
    "            return caption\n",
    "        else:\n",
    "            caption = self.data[idx]['eval_caption']\n",
    "            return self._words_to_we(self._tokenize_text(caption))\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.data[idx]['id']  #비디오의 고유 식별자 가져오기\n",
    "        # load 2d and 3d features (features are pooled over the time dimension)\n",
    "        feat_2d = F.normalize(torch.from_numpy(self.data[idx]['2d_pooled']).float(), dim=0) #2D 데이터 정규화\n",
    "        feat_3d = F.normalize(torch.from_numpy(self.data[idx]['3d_pooled']).float(), dim=0) #3D 데이터 정규화\n",
    "        video = torch.cat((feat_2d, feat_3d)) \n",
    "\n",
    "        # load audio and zero pad/truncate if necessary\n",
    "        audio = self.data[idx]['audio']  #오디오의 특징 가져오기\n",
    "        target_length = 1024 * self.num_frames_multiplier\n",
    "        nframes = audio.numpy().shape[1]\n",
    "        p = target_length - nframes #오디오의 길이를 확인하고 부족한 경우 패딩을 추가.\n",
    "        if p > 0:\n",
    "            audio = np.pad(audio, ((0,0),(0,p)), 'constant', constant_values=(0,0))\n",
    "        elif p < 0:\n",
    "            audio = audio[:,0:p]\n",
    "        audio = torch.FloatTensor(audio)\n",
    "\n",
    "        # choose a caption\n",
    "        caption=''\n",
    "        caption = self._get_caption(idx)\n",
    "\n",
    "        # category 추가\n",
    "        category = self.data[idx]['category']\n",
    "\n",
    "        return {'video': video, 'text': caption, 'video_id': video_id,\n",
    "                'audio': audio, 'nframes': nframes, 'category': category}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_path = 'C:/Users/heeryung/code/24w_deep_daiv/GoogleNews-vectors-negative300.bin'\n",
    "data_path = 'C:/Users/heeryung/code/24w_deep_daiv/msrvtt_category_test.pkl'\n",
    "\n",
    "we = KeyedVectors.load_word2vec_format(we_path, binary=True)\n",
    "dataset = MSRVTT_DataLoader(data_path=data_path, we=we)\n",
    "ori_dataset = pickle.load(open(data_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'video7020',\n",
       " 'audio': tensor([[-80.0000, -80.0000, -80.0000,  ..., -42.0759, -39.5331, -71.7272],\n",
       "         [-80.0000, -80.0000, -80.0000,  ..., -33.3933, -31.5323, -68.4606],\n",
       "         [-80.0000, -80.0000, -80.0000,  ..., -13.8896, -23.3922, -69.1881],\n",
       "         ...,\n",
       "         [-64.5946, -65.5943, -66.7466,  ..., -31.8602, -35.7037, -48.6106],\n",
       "         [-69.1042, -68.7961, -67.8672,  ..., -39.7601, -41.4094, -53.2567],\n",
       "         [-67.2767, -67.4139, -68.0604,  ..., -63.2213, -64.2628, -67.5640]]),\n",
       " '3d': array([[0.00738  , 0.007713 , 0.00536  , ..., 0.00956  , 0.02512  ,\n",
       "         0.0010195],\n",
       "        [0.0008926, 0.0002233, 0.00241  , ..., 0.00905  , 0.00473  ,\n",
       "         0.000507 ],\n",
       "        [0.000391 , 0.003115 , 0.005142 , ..., 0.004242 , 0.00395  ,\n",
       "         0.000588 ],\n",
       "        ...,\n",
       "        [0.       , 0.       , 0.02353  , ..., 0.001635 , 0.000309 ,\n",
       "         0.003887 ],\n",
       "        [0.       , 0.       , 0.00997  , ..., 0.01394  , 0.005226 ,\n",
       "         0.00604  ],\n",
       "        [0.0002294, 0.       , 0.004646 , ..., 0.013306 , 0.01353  ,\n",
       "         0.00805  ]], dtype=float16),\n",
       " 'caption': ['a person is preparing some art',\n",
       "  'a person making stuff out of clay',\n",
       "  'a woman  creating a fondant baby and flower',\n",
       "  'a woman constructing a sculpture with playdoh',\n",
       "  'a woman creates a baby out of craft supplies',\n",
       "  'a woman creates some crafts',\n",
       "  'a woman is crafting with clay',\n",
       "  'a woman is making crafts',\n",
       "  'a woman is making crafts',\n",
       "  'a woman makes a craft project',\n",
       "  'a woman makes crafts',\n",
       "  'a woman makes realistic looking leaves and flowers for a cake',\n",
       "  'a woman places a leaf on some dough and cuts a flower out of a different piece of dough',\n",
       "  'a woman wraps a baby doll in some fake leaves',\n",
       "  'showing a craft work',\n",
       "  'someone is doing a craft',\n",
       "  'someone is placing a leaf on a fake fetus of a baby and then proceeds to cutting up a flower',\n",
       "  'someone is showing some art',\n",
       "  'woman is showing steps to making fondant characters',\n",
       "  'wrapping a fake boy with a leaf'],\n",
       " '2d': array([[0.01049  , 0.00904  , 0.0628   , ..., 0.002438 , 0.00883  ,\n",
       "         0.02861  ],\n",
       "        [0.01985  , 0.00627  , 0.0718   , ..., 0.002113 , 0.0062   ,\n",
       "         0.02347  ],\n",
       "        [0.0237   , 0.002281 , 0.03152  , ..., 0.001285 , 0.003502 ,\n",
       "         0.02412  ],\n",
       "        ...,\n",
       "        [0.003313 , 0.008224 , 0.0144   , ..., 0.000238 , 0.001774 ,\n",
       "         0.0163   ],\n",
       "        [0.00438  , 0.00977  , 0.0808   , ..., 0.0008936, 0.0001458,\n",
       "         0.00509  ],\n",
       "        [0.007454 , 0.00899  , 0.0728   , ..., 0.001032 , 0.001414 ,\n",
       "         0.007492 ]], dtype=float16),\n",
       " 'eval_caption': 'a woman creating a fondant baby and flower',\n",
       " '2d_pooled': array([0.0237 , 0.02333, 0.0808 , ..., 0.00755, 0.00898, 0.02861],\n",
       "       dtype=float16),\n",
       " '3d_pooled': array([0.00738, 0.01482, 0.031  , ..., 0.01394, 0.0432 , 0.00805],\n",
       "       dtype=float16),\n",
       " 'eval_caption_ind': 2,\n",
       " 'category': 10}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in ori_dataset:\n",
    "    if data['id']=='video7061':\n",
    "        caption_7061 = data['caption']\n",
    "        eval_7061 = data['eval_caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in ori_dataset:\n",
    "    if data['id']=='video7118':\n",
    "        caption_7118 = data['caption']\n",
    "        eval_7118 = data['eval_caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a young girl in a horror movie is haunted'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_7118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'caption_7061': ['in a fish tank two red fishes are playing',\n",
       "  'music playing in the background showing fish swiming around',\n",
       "  'two orange and white fish are swimming together',\n",
       "  'two fishes are moving in a water aquarium',\n",
       "  'there are two fish floating in to the water',\n",
       "  'a fish tank with two gold fish and plants',\n",
       "  'goldfish chase each other around a blue tank to music',\n",
       "  'gold fishes are swimming in the blue water of aquarium',\n",
       "  'two orange and white fish frolic around an aquatic plant in a pool with a deep blue bottom',\n",
       "  'two coy fish swimming and eating as they play',\n",
       "  'two cute little gold fish are playing in water so nice to see',\n",
       "  'the orange fishes are present in the aquarium looking very beautifull',\n",
       "  'in the water fish seem to be eating from a plant',\n",
       "  'the orange fish are swimming in the aquarium',\n",
       "  'the orange fish are swimming in the aquarium',\n",
       "  'two little koi fish keep swimming next to each other',\n",
       "  'in an acquarium tank two fishes are playing and the colour of the fish is very different and it is so cute to watch ',\n",
       "  'fish swimming in the fish tank trying to mate each other',\n",
       "  'the fish featured in this video are orange in color',\n",
       "  'some red and white fish are swimming in a tank'],\n",
       " 'caption_7118': ['a clip from a movie is playing',\n",
       "  'a girl in bed knocking on the wall',\n",
       "  'a girl in bed',\n",
       "  'a girl is knocking on the wall',\n",
       "  'a girl knocking on a wall',\n",
       "  'a girl knocks on a wall and texts a friend',\n",
       "  'a girl lays in bed and uses her phone',\n",
       "  'a woman is laying in bed knocking on the wall',\n",
       "  'a woman is laying on a bed knocking wall behind her and writing a text message',\n",
       "  'a woman is sitting',\n",
       "  'a woman is sleeping in a bed in a bedroom and looking at a phone',\n",
       "  'a woman knocks on the wall of her bedroom and texts',\n",
       "  'a woman lying in bed texting',\n",
       "  'a woman sitting on a bed',\n",
       "  'a woman taps on her bedroom wall before texting on her phone',\n",
       "  'a young girl in a horror movie is haunted',\n",
       "  'girl knocking on the wall',\n",
       "  'girl sending sms to some one',\n",
       "  'scene from a tv show',\n",
       "  'someone texting on an old phone'],\n",
       " 'eval_caption_7118': ['a clip from a movie is playing',\n",
       "  'a girl in bed knocking on the wall',\n",
       "  'a girl in bed',\n",
       "  'a girl is knocking on the wall',\n",
       "  'a girl knocking on a wall',\n",
       "  'a girl knocks on a wall and texts a friend',\n",
       "  'a girl lays in bed and uses her phone',\n",
       "  'a woman is laying in bed knocking on the wall',\n",
       "  'a woman is laying on a bed knocking wall behind her and writing a text message',\n",
       "  'a woman is sitting',\n",
       "  'a woman is sleeping in a bed in a bedroom and looking at a phone',\n",
       "  'a woman knocks on the wall of her bedroom and texts',\n",
       "  'a woman lying in bed texting',\n",
       "  'a woman sitting on a bed',\n",
       "  'a woman taps on her bedroom wall before texting on her phone',\n",
       "  'a young girl in a horror movie is haunted',\n",
       "  'girl knocking on the wall',\n",
       "  'girl sending sms to some one',\n",
       "  'scene from a tv show',\n",
       "  'someone texting on an old phone']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption = {}\n",
    "caption['caption_7061'] = caption_7061\n",
    "caption['caption_7118'] = caption_7118\n",
    "caption['eval_caption_7118'] = caption_7118\n",
    "caption['eval_caption_7118'] = caption_7118\n",
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"C:/Users/heeryung/code/24w-Tri-Modalities/test_caption.json\", \"w\") as json_file:\n",
    "    json.dump(caption, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'video7020',\n",
       " 'audio': tensor([[-80.0000, -80.0000, -80.0000,  ..., -42.0759, -39.5331, -71.7272],\n",
       "         [-80.0000, -80.0000, -80.0000,  ..., -33.3933, -31.5323, -68.4606],\n",
       "         [-80.0000, -80.0000, -80.0000,  ..., -13.8896, -23.3922, -69.1881],\n",
       "         ...,\n",
       "         [-64.5946, -65.5943, -66.7466,  ..., -31.8602, -35.7037, -48.6106],\n",
       "         [-69.1042, -68.7961, -67.8672,  ..., -39.7601, -41.4094, -53.2567],\n",
       "         [-67.2767, -67.4139, -68.0604,  ..., -63.2213, -64.2628, -67.5640]]),\n",
       " '3d': array([[0.00738  , 0.007713 , 0.00536  , ..., 0.00956  , 0.02512  ,\n",
       "         0.0010195],\n",
       "        [0.0008926, 0.0002233, 0.00241  , ..., 0.00905  , 0.00473  ,\n",
       "         0.000507 ],\n",
       "        [0.000391 , 0.003115 , 0.005142 , ..., 0.004242 , 0.00395  ,\n",
       "         0.000588 ],\n",
       "        ...,\n",
       "        [0.       , 0.       , 0.02353  , ..., 0.001635 , 0.000309 ,\n",
       "         0.003887 ],\n",
       "        [0.       , 0.       , 0.00997  , ..., 0.01394  , 0.005226 ,\n",
       "         0.00604  ],\n",
       "        [0.0002294, 0.       , 0.004646 , ..., 0.013306 , 0.01353  ,\n",
       "         0.00805  ]], dtype=float16),\n",
       " 'caption': ['a person is preparing some art',\n",
       "  'a person making stuff out of clay',\n",
       "  'a woman  creating a fondant baby and flower',\n",
       "  'a woman constructing a sculpture with playdoh',\n",
       "  'a woman creates a baby out of craft supplies',\n",
       "  'a woman creates some crafts',\n",
       "  'a woman is crafting with clay',\n",
       "  'a woman is making crafts',\n",
       "  'a woman is making crafts',\n",
       "  'a woman makes a craft project',\n",
       "  'a woman makes crafts',\n",
       "  'a woman makes realistic looking leaves and flowers for a cake',\n",
       "  'a woman places a leaf on some dough and cuts a flower out of a different piece of dough',\n",
       "  'a woman wraps a baby doll in some fake leaves',\n",
       "  'showing a craft work',\n",
       "  'someone is doing a craft',\n",
       "  'someone is placing a leaf on a fake fetus of a baby and then proceeds to cutting up a flower',\n",
       "  'someone is showing some art',\n",
       "  'woman is showing steps to making fondant characters',\n",
       "  'wrapping a fake boy with a leaf'],\n",
       " '2d': array([[0.01049  , 0.00904  , 0.0628   , ..., 0.002438 , 0.00883  ,\n",
       "         0.02861  ],\n",
       "        [0.01985  , 0.00627  , 0.0718   , ..., 0.002113 , 0.0062   ,\n",
       "         0.02347  ],\n",
       "        [0.0237   , 0.002281 , 0.03152  , ..., 0.001285 , 0.003502 ,\n",
       "         0.02412  ],\n",
       "        ...,\n",
       "        [0.003313 , 0.008224 , 0.0144   , ..., 0.000238 , 0.001774 ,\n",
       "         0.0163   ],\n",
       "        [0.00438  , 0.00977  , 0.0808   , ..., 0.0008936, 0.0001458,\n",
       "         0.00509  ],\n",
       "        [0.007454 , 0.00899  , 0.0728   , ..., 0.001032 , 0.001414 ,\n",
       "         0.007492 ]], dtype=float16),\n",
       " 'eval_caption': 'a woman creating a fondant baby and flower',\n",
       " '2d_pooled': array([0.0237 , 0.02333, 0.0808 , ..., 0.00755, 0.00898, 0.02861],\n",
       "       dtype=float16),\n",
       " '3d_pooled': array([0.00738, 0.01482, 0.031  , ..., 0.01394, 0.0432 , 0.00805],\n",
       "       dtype=float16),\n",
       " 'eval_caption_ind': 2,\n",
       " 'category': 10}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_dataset[0] # 실제 video7020과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(in_planes, out_planes, width=9, stride=1, bias=False):\n",
    "    \"\"\"1xd convolution with padding\"\"\"\n",
    "    if width % 2 == 0:\n",
    "        pad_amt = int(width / 2)\n",
    "    else:\n",
    "        pad_amt = int((width - 1) / 2)\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=(1, width), stride=stride, padding=(0,pad_amt), bias=bias)\n",
    "\n",
    "class SpeechBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, width=9, stride=1, downsample=None):\n",
    "        super(SpeechBasicBlock, self).__init__()\n",
    "        self.conv1 = conv1d(inplanes, planes, width=width, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv1d(planes, planes, width=width)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResDavenet(nn.Module):\n",
    "    def __init__(self, feat_dim=40, block=SpeechBasicBlock, layers=[2, 2, 2, 2], layer_widths=[128, 128, 256, 512, 1024], convsize=9):\n",
    "        super(ResDavenet, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.inplanes = layer_widths[0]\n",
    "        self.batchnorm1 = nn.BatchNorm2d(1)\n",
    "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=(self.feat_dim,1), stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, layer_widths[1], layers[0], width=convsize, stride=2)\n",
    "        self.layer2 = self._make_layer(block, layer_widths[2], layers[1], width=convsize, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layer_widths[3], layers[2], width=convsize, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layer_widths[4], layers[3], width=convsize, stride=2)\n",
    "        if len(layers) == 6:\n",
    "            self.layer5 = self._make_layer(block, layer_widths[5], layers[4], width=convsize, stride=2)\n",
    "            self.layer6 = self._make_layer(block, layer_widths[6], layers[5], width=convsize, stride=2)\n",
    "        else:\n",
    "            self.layer5 = None\n",
    "            self.layer6 = None\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, width=9, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, width=width, stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, width=width, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        if self.layer5 is not None:\n",
    "            x = self.layer5(x)\n",
    "            x = self.layer6(x)\n",
    "        x = x.squeeze(2)\n",
    "        return x\n",
    "\n",
    "def load_DAVEnet(v2=True):\n",
    "    if v2:\n",
    "        audio_model = ResDavenet(feat_dim=40, layers=[2,2,1,1,1,1], convsize=9,\n",
    "                                 layer_widths=[128,128,256,512,1024,2048,4096])\n",
    "    else:\n",
    "        audio_model = ResDavenet(feat_dim=40, layers=[2, 2, 2, 2], convsize=9,\n",
    "                                 layer_widths=[128, 128, 256, 512, 1024])\n",
    "\n",
    "    return audio_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context_Gating(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super(Context_Gating, self).__init__()\n",
    "        self.fc = nn.Linear(dimension, dimension)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fc(x)          \n",
    "        x = torch.cat((x, x1), 1)   # 차원 = 2 * dimension\n",
    "        return F.glu(x, 1)       # 차원 = dimension , glu가 반만 이용\n",
    "    \n",
    "class Gated_Embedding_Unit(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        super(Gated_Embedding_Unit, self).__init__()\n",
    "        self.fc = nn.Linear(input_dimension, output_dimension)  # 차원 맞추기\n",
    "        self.cg = Context_Gating(output_dimension)              # Context Gating \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)         \n",
    "        x = self.cg(x)         \n",
    "        return x               \n",
    "    \n",
    "class projection_net(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim=1024,\n",
    "            video_dim=4096,\n",
    "            we_dim=300,\n",
    "            cross_attention=False\n",
    "    ):\n",
    "        super(projection_net, self).__init__()\n",
    "        self.cross_attention = cross_attention\n",
    "\n",
    "    # Fuse적용 X\n",
    "        self.DAVEnet = load_DAVEnet(v2=True)\n",
    "        self.GU_audio = Gated_Embedding_Unit(4096, embed_dim)\n",
    "        self.GU_video = Gated_Embedding_Unit(video_dim, embed_dim)\n",
    "        self.GU_text_captions = Gated_Embedding_Unit(we_dim, embed_dim)\n",
    "\n",
    "    def forward(self, video, audio_input, nframes, text=None):\n",
    "        audio = self.DAVEnet(audio_input) # [16, 1024, 320]\n",
    "        audio = audio.permute(0,2,1)\n",
    "\n",
    "        # text = self.GU_text_captions(self.text_pooling_caption(text)) # [16,30,300] -> [16,4096]\n",
    "        text = self.GU_text_captions(text)\n",
    "        audio = self.GU_audio(audio) \n",
    "        video = self.GU_video(video) \n",
    "        return audio, text, video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., init_values=None,\n",
    "            drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, use_softmax=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.cross_attn = MultiHeadCrossAttention(d_model=dim, n_head=num_heads, use_softmax=use_softmax)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, k, q, attention_mask=None):\n",
    "        output = q + self.drop_path(self.cross_attn(self.norm1(k), self.norm1(q)))#, attention_mask))  ##### 1) query만 residual\n",
    "        output = output + self.drop_path(self.mlp(self.norm2(output)))\n",
    "        return output\n",
    "\n",
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self, use_softmax):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.use_softmax = use_softmax\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, e=1e-12):\n",
    "        # input size: [batch_size, head, length, d_tensor]\n",
    "        batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "        k_t = k.transpose(2,3)\n",
    "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "        if self.use_softmax:\n",
    "            score = self.softmax(score)  #[0,1]\n",
    "        v = score @ v\n",
    "\n",
    "        return v, score \n",
    "    \n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, use_softmax):\n",
    "        super(MultiHeadCrossAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.use_softmax = use_softmax\n",
    "        self.attention = ScaleDotProductAttention(use_softmax)\n",
    "\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, k, q):\n",
    "        v = k\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "        out, attention = self.attention(q, k, v)\n",
    "        out = self.concat(out)\n",
    "        out = self.w_concat(out)\n",
    "        return out \n",
    "\n",
    "    def split(self, tensor):\n",
    "        # [batch_size, length, d_model] -> [batch_size, head, length, d_model]\n",
    "        batch_size, length, d_model = tensor.size()\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1,2)\n",
    "        return tensor \n",
    "    \n",
    "    def concat(self, tensor):\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        tensor = tensor.transpose(1,2).contiguous().view(batch_size, length, self.d_model)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, depth=1, num_heads=64, mlp_ratio=1, qkv_bias=True,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=None,\n",
    "                 act_layer=None,\n",
    "                 use_cls_token=True,\n",
    "                 num_classes=20,\n",
    "                 use_softmax=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if use_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "\n",
    "        self.masking_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            FusionBlock(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer, use_softmax=use_softmax\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "\n",
    "        self.norm = norm_layer(embed_dim) # TODO: not needed, remove?\n",
    "\n",
    "        self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, key, query, key_modal='', query_modal=''):\n",
    "        token_k = key\n",
    "        token_q = query\n",
    "\n",
    "        # FusionBlock (cross attnetion)\n",
    "        for block in self.blocks:\n",
    "            tokens = block(token_k, token_q)\n",
    "        output = tokens\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer_1 = nn.Linear(latent_dim, 256)\n",
    "        self.layer_2 = nn.Linear(256, 128)\n",
    "        self.layer_3 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EverythingAtOnceModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 embed_dim=1024,\n",
    "                 video_embed_dim=4096,\n",
    "                 text_embed_dim=300,\n",
    "                 video_max_tokens=None,\n",
    "                 text_max_tokens=None,\n",
    "                 audio_max_num_STFT_frames=None,\n",
    "                 projection_dim=6144,\n",
    "                 projection='gated',\n",
    "                 strategy_audio_pooling='none',\n",
    "                 davenet_v2=True,\n",
    "                 individual_projections=True,\n",
    "                 use_positional_emb=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.use_softmax = True\n",
    "        self.use_cls_token = False\n",
    "        self.num_classes = 20\n",
    "\n",
    "        self.fusion = FusionTransformer(embed_dim=self.embed_dim, use_softmax=self.use_softmax, use_cls_token=self.use_cls_token, num_classes = self.num_classes)\n",
    "\n",
    "        self.token_projection = 'projection_net'\n",
    "\n",
    "        self.individual_projections = individual_projections\n",
    "        self.use_positional_emb = use_positional_emb\n",
    "        self.strategy_audio_pooling = strategy_audio_pooling\n",
    "\n",
    "        self.video_norm_layer = nn.LayerNorm(self.embed_dim, eps=1e-6)\n",
    "        self.text_norm_layer = nn.LayerNorm(self.embed_dim, eps=1e-6)\n",
    "        self.audio_norm_layer = nn.LayerNorm(self.embed_dim, eps=1e-6)\n",
    "        self.norm_layer = nn.LayerNorm(self.embed_dim, eps=1e-6)\n",
    "\n",
    "        # audio token preprocess\n",
    "        # self.davenet = load_DAVEnet(v2=davenet_v2)\n",
    "        self.token_proj = projection_net(embed_dim=self.embed_dim)\n",
    "\n",
    "        # self.commonencoder=CommonEncoder(common_dim=self.embed_dim, latent_dim=512)\n",
    "        self.classifier1 = Classifier(latent_dim=self.embed_dim, num_classes=self.num_classes)\n",
    "        self.classifier2 = Classifier(latent_dim=self.embed_dim, num_classes=self.num_classes)\n",
    "        self.classifier3 = Classifier(latent_dim=self.embed_dim, num_classes=self.num_classes)\n",
    "    \n",
    "    def extract_tokens(self, video, audio, text, nframes):\n",
    "        audio, text, video = self.token_proj(video, audio, nframes, text)\n",
    "        return audio, text, video\n",
    "\n",
    "    def forward(self, video, audio, nframes, text, category, force_cross_modal=False):\n",
    "        audio_raw_embed, text_raw_embed, video_raw_embed = self.extract_tokens(video, audio, text, nframes)\n",
    "        video_raw_embed = torch.unsqueeze(video_raw_embed, 1) # ([16, 1, 1024] [16, 80, 1024] [16, 30, 1024]\n",
    "\n",
    "\n",
    "        ## Visual - Audio\n",
    "        va = self.fusion(key=video_raw_embed, query=audio_raw_embed) # [16, 80, 20]\n",
    "        va = self.classifier1(va.mean(dim=1)) \n",
    "\n",
    "        ## Audio - Text\n",
    "        at = self.fusion(key=audio_raw_embed, query=text_raw_embed) # [16, 30, 20]\n",
    "        at = self.classifier2(at.mean(dim=1))\n",
    "\n",
    "        ## Text - Video\n",
    "        tv = self.fusion(key=text_raw_embed, query=video_raw_embed) # [16, 1, 20]\n",
    "        tv = self.classifier3(tv.mean(dim=1))\n",
    "\n",
    "        return va ,at, tv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EverythingAtOnceModel:\n\tUnexpected key(s) in state_dict: \"davenet.batchnorm1.weight\", \"davenet.batchnorm1.bias\", \"davenet.batchnorm1.running_mean\", \"davenet.batchnorm1.running_var\", \"davenet.batchnorm1.num_batches_tracked\", \"davenet.conv1.weight\", \"davenet.bn1.weight\", \"davenet.bn1.bias\", \"davenet.bn1.running_mean\", \"davenet.bn1.running_var\", \"davenet.bn1.num_batches_tracked\", \"davenet.layer1.0.conv1.weight\", \"davenet.layer1.0.bn1.weight\", \"davenet.layer1.0.bn1.bias\", \"davenet.layer1.0.bn1.running_mean\", \"davenet.layer1.0.bn1.running_var\", \"davenet.layer1.0.bn1.num_batches_tracked\", \"davenet.layer1.0.conv2.weight\", \"davenet.layer1.0.bn2.weight\", \"davenet.layer1.0.bn2.bias\", \"davenet.layer1.0.bn2.running_mean\", \"davenet.layer1.0.bn2.running_var\", \"davenet.layer1.0.bn2.num_batches_tracked\", \"davenet.layer1.0.downsample.0.weight\", \"davenet.layer1.0.downsample.1.weight\", \"davenet.layer1.0.downsample.1.bias\", \"davenet.layer1.0.downsample.1.running_mean\", \"davenet.layer1.0.downsample.1.running_var\", \"davenet.layer1.0.downsample.1.num_batches_tracked\", \"davenet.layer1.1.conv1.weight\", \"davenet.layer1.1.bn1.weight\", \"davenet.layer1.1.bn1.bias\", \"davenet.layer1.1.bn1.running_mean\", \"davenet.layer1.1.bn1.running_var\", \"davenet.layer1.1.bn1.num_batches_tracked\", \"davenet.layer1.1.conv2.weight\", \"davenet.layer1.1.bn2.weight\", \"davenet.layer1.1.bn2.bias\", \"davenet.layer1.1.bn2.running_mean\", \"davenet.layer1.1.bn2.running_var\", \"davenet.layer1.1.bn2.num_batches_tracked\", \"davenet.layer2.0.conv1.weight\", \"davenet.layer2.0.bn1.weight\", \"davenet.layer2.0.bn1.bias\", \"davenet.layer2.0.bn1.running_mean\", \"davenet.layer2.0.bn1.running_var\", \"davenet.layer2.0.bn1.num_batches_tracked\", \"davenet.layer2.0.conv2.weight\", \"davenet.layer2.0.bn2.weight\", \"davenet.layer2.0.bn2.bias\", \"davenet.layer2.0.bn2.running_mean\", \"davenet.layer2.0.bn2.running_var\", \"davenet.layer2.0.bn2.num_batches_tracked\", \"davenet.layer2.0.downsample.0.weight\", \"davenet.layer2.0.downsample.1.weight\", \"davenet.layer2.0.downsample.1.bias\", \"davenet.layer2.0.downsample.1.running_mean\", \"davenet.layer2.0.downsample.1.running_var\", \"davenet.layer2.0.downsample.1.num_batches_tracked\", \"davenet.layer2.1.conv1.weight\", \"davenet.layer2.1.bn1.weight\", \"davenet.layer2.1.bn1.bias\", \"davenet.layer2.1.bn1.running_mean\", \"davenet.layer2.1.bn1.running_var\", \"davenet.layer2.1.bn1.num_batches_tracked\", \"davenet.layer2.1.conv2.weight\", \"davenet.layer2.1.bn2.weight\", \"davenet.layer2.1.bn2.bias\", \"davenet.layer2.1.bn2.running_mean\", \"davenet.layer2.1.bn2.running_var\", \"davenet.layer2.1.bn2.num_batches_tracked\", \"davenet.layer3.0.conv1.weight\", \"davenet.layer3.0.bn1.weight\", \"davenet.layer3.0.bn1.bias\", \"davenet.layer3.0.bn1.running_mean\", \"davenet.layer3.0.bn1.running_var\", \"davenet.layer3.0.bn1.num_batches_tracked\", \"davenet.layer3.0.conv2.weight\", \"davenet.layer3.0.bn2.weight\", \"davenet.layer3.0.bn2.bias\", \"davenet.layer3.0.bn2.running_mean\", \"davenet.layer3.0.bn2.running_var\", \"davenet.layer3.0.bn2.num_batches_tracked\", \"davenet.layer3.0.downsample.0.weight\", \"davenet.layer3.0.downsample.1.weight\", \"davenet.layer3.0.downsample.1.bias\", \"davenet.layer3.0.downsample.1.running_mean\", \"davenet.layer3.0.downsample.1.running_var\", \"davenet.layer3.0.downsample.1.num_batches_tracked\", \"davenet.layer4.0.conv1.weight\", \"davenet.layer4.0.bn1.weight\", \"davenet.layer4.0.bn1.bias\", \"davenet.layer4.0.bn1.running_mean\", \"davenet.layer4.0.bn1.running_var\", \"davenet.layer4.0.bn1.num_batches_tracked\", \"davenet.layer4.0.conv2.weight\", \"davenet.layer4.0.bn2.weight\", \"davenet.layer4.0.bn2.bias\", \"davenet.layer4.0.bn2.running_mean\", \"davenet.layer4.0.bn2.running_var\", \"davenet.layer4.0.bn2.num_batches_tracked\", \"davenet.layer4.0.downsample.0.weight\", \"davenet.layer4.0.downsample.1.weight\", \"davenet.layer4.0.downsample.1.bias\", \"davenet.layer4.0.downsample.1.running_mean\", \"davenet.layer4.0.downsample.1.running_var\", \"davenet.layer4.0.downsample.1.num_batches_tracked\", \"davenet.layer5.0.conv1.weight\", \"davenet.layer5.0.bn1.weight\", \"davenet.layer5.0.bn1.bias\", \"davenet.layer5.0.bn1.running_mean\", \"davenet.layer5.0.bn1.running_var\", \"davenet.layer5.0.bn1.num_batches_tracked\", \"davenet.layer5.0.conv2.weight\", \"davenet.layer5.0.bn2.weight\", \"davenet.layer5.0.bn2.bias\", \"davenet.layer5.0.bn2.running_mean\", \"davenet.layer5.0.bn2.running_var\", \"davenet.layer5.0.bn2.num_batches_tracked\", \"davenet.layer5.0.downsample.0.weight\", \"davenet.layer5.0.downsample.1.weight\", \"davenet.layer5.0.downsample.1.bias\", \"davenet.layer5.0.downsample.1.running_mean\", \"davenet.layer5.0.downsample.1.running_var\", \"davenet.layer5.0.downsample.1.num_batches_tracked\", \"davenet.layer6.0.conv1.weight\", \"davenet.layer6.0.bn1.weight\", \"davenet.layer6.0.bn1.bias\", \"davenet.layer6.0.bn1.running_mean\", \"davenet.layer6.0.bn1.running_var\", \"davenet.layer6.0.bn1.num_batches_tracked\", \"davenet.layer6.0.conv2.weight\", \"davenet.layer6.0.bn2.weight\", \"davenet.layer6.0.bn2.bias\", \"davenet.layer6.0.bn2.running_mean\", \"davenet.layer6.0.bn2.running_var\", \"davenet.layer6.0.bn2.num_batches_tracked\", \"davenet.layer6.0.downsample.0.weight\", \"davenet.layer6.0.downsample.1.weight\", \"davenet.layer6.0.downsample.1.bias\", \"davenet.layer6.0.downsample.1.running_mean\", \"davenet.layer6.0.downsample.1.running_var\", \"davenet.layer6.0.downsample.1.num_batches_tracked\", \"commonencoder.feature_extractor.0.weight\", \"commonencoder.feature_extractor.0.bias\", \"commonencoder.feature_extractor.2.weight\", \"commonencoder.feature_extractor.2.bias\", \"commonencoder.feature_extractor.4.weight\", \"commonencoder.feature_extractor.4.bias\". \n\tsize mismatch for classifier1.layer_1.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for classifier2.layer_1.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for classifier3.layer_1.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_path)\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m EverythingAtOnceModel(args)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\heeryung\\anaconda3\\envs\\jiwon2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EverythingAtOnceModel:\n\tUnexpected key(s) in state_dict: \"davenet.batchnorm1.weight\", \"davenet.batchnorm1.bias\", \"davenet.batchnorm1.running_mean\", \"davenet.batchnorm1.running_var\", \"davenet.batchnorm1.num_batches_tracked\", \"davenet.conv1.weight\", \"davenet.bn1.weight\", \"davenet.bn1.bias\", \"davenet.bn1.running_mean\", \"davenet.bn1.running_var\", \"davenet.bn1.num_batches_tracked\", \"davenet.layer1.0.conv1.weight\", \"davenet.layer1.0.bn1.weight\", \"davenet.layer1.0.bn1.bias\", \"davenet.layer1.0.bn1.running_mean\", \"davenet.layer1.0.bn1.running_var\", \"davenet.layer1.0.bn1.num_batches_tracked\", \"davenet.layer1.0.conv2.weight\", \"davenet.layer1.0.bn2.weight\", \"davenet.layer1.0.bn2.bias\", \"davenet.layer1.0.bn2.running_mean\", \"davenet.layer1.0.bn2.running_var\", \"davenet.layer1.0.bn2.num_batches_tracked\", \"davenet.layer1.0.downsample.0.weight\", \"davenet.layer1.0.downsample.1.weight\", \"davenet.layer1.0.downsample.1.bias\", \"davenet.layer1.0.downsample.1.running_mean\", \"davenet.layer1.0.downsample.1.running_var\", \"davenet.layer1.0.downsample.1.num_batches_tracked\", \"davenet.layer1.1.conv1.weight\", \"davenet.layer1.1.bn1.weight\", \"davenet.layer1.1.bn1.bias\", \"davenet.layer1.1.bn1.running_mean\", \"davenet.layer1.1.bn1.running_var\", \"davenet.layer1.1.bn1.num_batches_tracked\", \"davenet.layer1.1.conv2.weight\", \"davenet.layer1.1.bn2.weight\", \"davenet.layer1.1.bn2.bias\", \"davenet.layer1.1.bn2.running_mean\", \"davenet.layer1.1.bn2.running_var\", \"davenet.layer1.1.bn2.num_batches_tracked\", \"davenet.layer2.0.conv1.weight\", \"davenet.layer2.0.bn1.weight\", \"davenet.layer2.0.bn1.bias\", \"davenet.layer2.0.bn1.running_mean\", \"davenet.layer2.0.bn1.running_var\", \"davenet.layer2.0.bn1.num_batches_tracked\", \"davenet.layer2.0.conv2.weight\", \"davenet.layer2.0.bn2.weight\", \"davenet.layer2.0.bn2.bias\", \"davenet.layer2.0.bn2.running_mean\", \"davenet.layer2.0.bn2.running_var\", \"davenet.layer2.0.bn2.num_batches_tracked\", \"davenet.layer2.0.downsample.0.weight\", \"davenet.layer2.0.downsample.1.weight\", \"davenet.layer2.0.downsample.1.bias\", \"davenet.layer2.0.downsample.1.running_mean\", \"davenet.layer2.0.downsample.1.running_var\", \"davenet.layer2.0.downsample.1.num_batches_tracked\", \"davenet.layer2.1.conv1.weight\", \"davenet.layer2.1.bn1.weight\", \"davenet.layer2.1.bn1.bias\", \"davenet.layer2.1.bn1.running_mean\", \"davenet.layer2.1.bn1.running_var\", \"davenet.layer2.1.bn1.num_batches_tracked\", \"davenet.layer2.1.conv2.weight\", \"davenet.layer2.1.bn2.weight\", \"davenet.layer2.1.bn2.bias\", \"davenet.layer2.1.bn2.running_mean\", \"davenet.layer2.1.bn2.running_var\", \"davenet.layer2.1.bn2.num_batches_tracked\", \"davenet.layer3.0.conv1.weight\", \"davenet.layer3.0.bn1.weight\", \"davenet.layer3.0.bn1.bias\", \"davenet.layer3.0.bn1.running_mean\", \"davenet.layer3.0.bn1.running_var\", \"davenet.layer3.0.bn1.num_batches_tracked\", \"davenet.layer3.0.conv2.weight\", \"davenet.layer3.0.bn2.weight\", \"davenet.layer3.0.bn2.bias\", \"davenet.layer3.0.bn2.running_mean\", \"davenet.layer3.0.bn2.running_var\", \"davenet.layer3.0.bn2.num_batches_tracked\", \"davenet.layer3.0.downsample.0.weight\", \"davenet.layer3.0.downsample.1.weight\", \"davenet.layer3.0.downsample.1.bias\", \"davenet.layer3.0.downsample.1.running_mean\", \"davenet.layer3.0.downsample.1.running_var\", \"davenet.layer3.0.downsample.1.num_batches_tracked\", \"davenet.layer4.0.conv1.weight\", \"davenet.layer4.0.bn1.weight\", \"davenet.layer4.0.bn1.bias\", \"davenet.layer4.0.bn1.running_mean\", \"davenet.layer4.0.bn1.running_var\", \"davenet.layer4.0.bn1.num_batches_tracked\", \"davenet.layer4.0.conv2.weight\", \"davenet.layer4.0.bn2.weight\", \"davenet.layer4.0.bn2.bias\", \"davenet.layer4.0.bn2.running_mean\", \"davenet.layer4.0.bn2.running_var\", \"davenet.layer4.0.bn2.num_batches_tracked\", \"davenet.layer4.0.downsample.0.weight\", \"davenet.layer4.0.downsample.1.weight\", \"davenet.layer4.0.downsample.1.bias\", \"davenet.layer4.0.downsample.1.running_mean\", \"davenet.layer4.0.downsample.1.running_var\", \"davenet.layer4.0.downsample.1.num_batches_tracked\", \"davenet.layer5.0.conv1.weight\", \"davenet.layer5.0.bn1.weight\", \"davenet.layer5.0.bn1.bias\", \"davenet.layer5.0.bn1.running_mean\", \"davenet.layer5.0.bn1.running_var\", \"davenet.layer5.0.bn1.num_batches_tracked\", \"davenet.layer5.0.conv2.weight\", \"davenet.layer5.0.bn2.weight\", \"davenet.layer5.0.bn2.bias\", \"davenet.layer5.0.bn2.running_mean\", \"davenet.layer5.0.bn2.running_var\", \"davenet.layer5.0.bn2.num_batches_tracked\", \"davenet.layer5.0.downsample.0.weight\", \"davenet.layer5.0.downsample.1.weight\", \"davenet.layer5.0.downsample.1.bias\", \"davenet.layer5.0.downsample.1.running_mean\", \"davenet.layer5.0.downsample.1.running_var\", \"davenet.layer5.0.downsample.1.num_batches_tracked\", \"davenet.layer6.0.conv1.weight\", \"davenet.layer6.0.bn1.weight\", \"davenet.layer6.0.bn1.bias\", \"davenet.layer6.0.bn1.running_mean\", \"davenet.layer6.0.bn1.running_var\", \"davenet.layer6.0.bn1.num_batches_tracked\", \"davenet.layer6.0.conv2.weight\", \"davenet.layer6.0.bn2.weight\", \"davenet.layer6.0.bn2.bias\", \"davenet.layer6.0.bn2.running_mean\", \"davenet.layer6.0.bn2.running_var\", \"davenet.layer6.0.bn2.num_batches_tracked\", \"davenet.layer6.0.downsample.0.weight\", \"davenet.layer6.0.downsample.1.weight\", \"davenet.layer6.0.downsample.1.bias\", \"davenet.layer6.0.downsample.1.running_mean\", \"davenet.layer6.0.downsample.1.running_var\", \"davenet.layer6.0.downsample.1.num_batches_tracked\", \"commonencoder.feature_extractor.0.weight\", \"commonencoder.feature_extractor.0.bias\", \"commonencoder.feature_extractor.2.weight\", \"commonencoder.feature_extractor.2.bias\", \"commonencoder.feature_extractor.4.weight\", \"commonencoder.feature_extractor.4.bias\". \n\tsize mismatch for classifier1.layer_1.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for classifier2.layer_1.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for classifier3.layer_1.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024])."
     ]
    }
   ],
   "source": [
    "import argparse \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--we_path', default='C:/Users/heeryung/code/24w_deep_daiv/GoogleNews-vectors-negative300.bin', type=str)\n",
    "parser.add_argument('--data_path', default='C:/Users/heeryung/code/24w_deep_daiv/msrvtt_category_test.pkl', type=str)\n",
    "parser.add_argument('--checkpoint_path', default='D:/download/epoch200.pth', type=str)\n",
    "parser.add_argument('--token_projection', default='projection_net', type=str) \n",
    "parser.add_argument('--use_softmax', default=True, type=bool) \n",
    "parser.add_argument('--use_cls_token', default=False, type=bool) \n",
    "parser.add_argument('--num_classes', default=20, type=int) \n",
    "parser.add_argument('--batch_size', default=16, type=int) \n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "ckpt_path = 'D:/Download/epoch200.pth'\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "model = EverythingAtOnceModel(args).cuda()\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "video = data['video']#.cuda()\n",
    "audio = data['audio']#.cuda()\n",
    "text = data['text']#.cuda()\n",
    "nframes = data['nframes']#.cuda()\n",
    "category = data['category']#.cuda()\n",
    "\n",
    "video = video.view(-1, video.shape[-1])\n",
    "audio = audio.view(-1, audio.shape[-2], audio.shape[-1])\n",
    "text = text.view(-1, text.shape[-2], text.shape[-1])\n",
    "\n",
    "pred = model(video, audio, nframes, text, category)\n",
    "pred_category = torch.argmax(pred, dim=1) \n",
    "# accuracy = torch.mean((pred_category == category).float()) \n",
    "print(\"Real category:\", category, 'Pred category:', pred_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiwon2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
