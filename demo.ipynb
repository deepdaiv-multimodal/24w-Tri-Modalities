{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비디오 로드\n",
    "# 비디오 피처 뽑기\n",
    "# 오디오 -> spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from functools import partial\n",
    "from timm.models.vision_transformer import DropPath, Mlp, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSRVTT_DataLoader(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path,\n",
    "            we, #word embewdding\n",
    "            we_dim=300,\n",
    "            max_words=30,\n",
    "            num_frames_multiplier=5, #오디에 데이터 길이 조절용\n",
    "            training=True\n",
    "    ):\n",
    "        self.data = pickle.load(open(data_path, 'rb')) #pkl파일을 바이트 스트림(이진 모드)\n",
    "        self.we = we\n",
    "        self.we_dim = we_dim\n",
    "        self.max_words = max_words\n",
    "        self.max_video = 30\n",
    "        self.num_frames_multiplier = num_frames_multiplier\n",
    "        self.training = training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _zero_pad_tensor(self, tensor, size):  #입력 텐서의 크기를 고정된 크기로 만들기\n",
    "        if len(tensor) >= size:\n",
    "            return tensor[:size]\n",
    "        else:\n",
    "            zero = np.zeros((size - len(tensor), self.we_dim), dtype=np.float32)\n",
    "            return np.concatenate((tensor, zero), axis=0)\n",
    "\n",
    "    def _tokenize_text(self, sentence):  #텍스트를 단어 또는 부분 문자열로 분할\n",
    "        w = re.findall(r\"[\\w']+\", str(sentence))\n",
    "        return w\n",
    "\n",
    "    def _words_to_we(self, words):  #단어를 임베딩 벡터로 변환\n",
    "        # words = [word for word in words if word in self.we.vocab]\n",
    "        words = [word for word in words if word in self.we.key_to_index]\n",
    "        if words: #해당 단어가 임베딩 모델에 존재할 때 벡터 추출(학습 한 것만)\n",
    "            we = self._zero_pad_tensor(self.we[words], self.max_words)\n",
    "            return torch.from_numpy(we)\n",
    "        else:\n",
    "            return torch.zeros(self.max_words, self.we_dim)\n",
    "\n",
    "    def _get_caption(self, idx):\n",
    "        \"\"\"Chooses random caption if training. Uses set caption if evaluating.\"\"\"\n",
    "        if self.training: #훈련중일 경우 무작위 caption 가져오기\n",
    "            captions = self.data[idx]['caption']\n",
    "            caption = self._words_to_we(self._tokenize_text(random.choice(captions)))\n",
    "            return caption\n",
    "        else:\n",
    "            caption = self.data[idx]['eval_caption']\n",
    "            return self._words_to_we(self._tokenize_text(caption))\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.data[idx]['id']  #비디오의 고유 식별자 가져오기\n",
    "        # load 2d and 3d features (features are pooled over the time dimension)\n",
    "        feat_2d = F.normalize(torch.from_numpy(self.data[idx]['2d_pooled']).float(), dim=0) #2D 데이터 정규화\n",
    "        feat_3d = F.normalize(torch.from_numpy(self.data[idx]['3d_pooled']).float(), dim=0) #3D 데이터 정규화\n",
    "        video = torch.cat((feat_2d, feat_3d)) \n",
    "\n",
    "        # load audio and zero pad/truncate if necessary\n",
    "        audio = self.data[idx]['audio']  #오디오의 특징 가져오기\n",
    "        target_length = 1024 * self.num_frames_multiplier\n",
    "        nframes = audio.numpy().shape[1]\n",
    "        p = target_length - nframes #오디오의 길이를 확인하고 부족한 경우 패딩을 추가.\n",
    "        if p > 0:\n",
    "            audio = np.pad(audio, ((0,0),(0,p)), 'constant', constant_values=(0,0))\n",
    "        elif p < 0:\n",
    "            audio = audio[:,0:p]\n",
    "        audio = torch.FloatTensor(audio)\n",
    "\n",
    "        # choose a caption\n",
    "        caption=''\n",
    "        caption = self._get_caption(idx)\n",
    "\n",
    "        # category 추가\n",
    "        category = self.data[idx]['category']\n",
    "\n",
    "        return {'video': video, 'text': caption, 'video_id': video_id,\n",
    "                'audio': audio, 'nframes': nframes, 'category': category}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_path = 'C:/Users/heeryung/code/24w_deep_daiv/GoogleNews-vectors-negative300.bin'\n",
    "data_path = 'C:/Users/heeryung/code/24w_deep_daiv/msrvtt_category_test.pkl'\n",
    "\n",
    "we = KeyedVectors.load_word2vec_format(we_path, binary=True)\n",
    "dataset = MSRVTT_DataLoader(data_path=data_path, we=we)\n",
    "ori_dataset = pickle.load(open(data_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'video7020',\n",
       " 'audio': tensor([[-80.0000, -80.0000, -80.0000,  ..., -42.0759, -39.5331, -71.7272],\n",
       "         [-80.0000, -80.0000, -80.0000,  ..., -33.3933, -31.5323, -68.4606],\n",
       "         [-80.0000, -80.0000, -80.0000,  ..., -13.8896, -23.3922, -69.1881],\n",
       "         ...,\n",
       "         [-64.5946, -65.5943, -66.7466,  ..., -31.8602, -35.7037, -48.6106],\n",
       "         [-69.1042, -68.7961, -67.8672,  ..., -39.7601, -41.4094, -53.2567],\n",
       "         [-67.2767, -67.4139, -68.0604,  ..., -63.2213, -64.2628, -67.5640]]),\n",
       " '3d': array([[0.00738  , 0.007713 , 0.00536  , ..., 0.00956  , 0.02512  ,\n",
       "         0.0010195],\n",
       "        [0.0008926, 0.0002233, 0.00241  , ..., 0.00905  , 0.00473  ,\n",
       "         0.000507 ],\n",
       "        [0.000391 , 0.003115 , 0.005142 , ..., 0.004242 , 0.00395  ,\n",
       "         0.000588 ],\n",
       "        ...,\n",
       "        [0.       , 0.       , 0.02353  , ..., 0.001635 , 0.000309 ,\n",
       "         0.003887 ],\n",
       "        [0.       , 0.       , 0.00997  , ..., 0.01394  , 0.005226 ,\n",
       "         0.00604  ],\n",
       "        [0.0002294, 0.       , 0.004646 , ..., 0.013306 , 0.01353  ,\n",
       "         0.00805  ]], dtype=float16),\n",
       " 'caption': ['a person is preparing some art',\n",
       "  'a person making stuff out of clay',\n",
       "  'a woman  creating a fondant baby and flower',\n",
       "  'a woman constructing a sculpture with playdoh',\n",
       "  'a woman creates a baby out of craft supplies',\n",
       "  'a woman creates some crafts',\n",
       "  'a woman is crafting with clay',\n",
       "  'a woman is making crafts',\n",
       "  'a woman is making crafts',\n",
       "  'a woman makes a craft project',\n",
       "  'a woman makes crafts',\n",
       "  'a woman makes realistic looking leaves and flowers for a cake',\n",
       "  'a woman places a leaf on some dough and cuts a flower out of a different piece of dough',\n",
       "  'a woman wraps a baby doll in some fake leaves',\n",
       "  'showing a craft work',\n",
       "  'someone is doing a craft',\n",
       "  'someone is placing a leaf on a fake fetus of a baby and then proceeds to cutting up a flower',\n",
       "  'someone is showing some art',\n",
       "  'woman is showing steps to making fondant characters',\n",
       "  'wrapping a fake boy with a leaf'],\n",
       " '2d': array([[0.01049  , 0.00904  , 0.0628   , ..., 0.002438 , 0.00883  ,\n",
       "         0.02861  ],\n",
       "        [0.01985  , 0.00627  , 0.0718   , ..., 0.002113 , 0.0062   ,\n",
       "         0.02347  ],\n",
       "        [0.0237   , 0.002281 , 0.03152  , ..., 0.001285 , 0.003502 ,\n",
       "         0.02412  ],\n",
       "        ...,\n",
       "        [0.003313 , 0.008224 , 0.0144   , ..., 0.000238 , 0.001774 ,\n",
       "         0.0163   ],\n",
       "        [0.00438  , 0.00977  , 0.0808   , ..., 0.0008936, 0.0001458,\n",
       "         0.00509  ],\n",
       "        [0.007454 , 0.00899  , 0.0728   , ..., 0.001032 , 0.001414 ,\n",
       "         0.007492 ]], dtype=float16),\n",
       " 'eval_caption': 'a woman creating a fondant baby and flower',\n",
       " '2d_pooled': array([0.0237 , 0.02333, 0.0808 , ..., 0.00755, 0.00898, 0.02861],\n",
       "       dtype=float16),\n",
       " '3d_pooled': array([0.00738, 0.01482, 0.031  , ..., 0.01394, 0.0432 , 0.00805],\n",
       "       dtype=float16),\n",
       " 'eval_caption_ind': 2,\n",
       " 'category': 10}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_dataset[0] # 실제 video7020과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(in_planes, out_planes, width=9, stride=1, bias=False):\n",
    "    \"\"\"1xd convolution with padding\"\"\"\n",
    "    if width % 2 == 0:\n",
    "        pad_amt = int(width / 2)\n",
    "    else:\n",
    "        pad_amt = int((width - 1) / 2)\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=(1, width), stride=stride, padding=(0,pad_amt), bias=bias)\n",
    "\n",
    "class SpeechBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, width=9, stride=1, downsample=None):\n",
    "        super(SpeechBasicBlock, self).__init__()\n",
    "        self.conv1 = conv1d(inplanes, planes, width=width, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv1d(planes, planes, width=width)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResDavenet(nn.Module):\n",
    "    def __init__(self, feat_dim=40, block=SpeechBasicBlock, layers=[2, 2, 2, 2], layer_widths=[128, 128, 256, 512, 1024], convsize=9):\n",
    "        super(ResDavenet, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.inplanes = layer_widths[0]\n",
    "        self.batchnorm1 = nn.BatchNorm2d(1)\n",
    "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=(self.feat_dim,1), stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, layer_widths[1], layers[0], width=convsize, stride=2)\n",
    "        self.layer2 = self._make_layer(block, layer_widths[2], layers[1], width=convsize, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layer_widths[3], layers[2], width=convsize, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layer_widths[4], layers[3], width=convsize, stride=2)\n",
    "        if len(layers) == 6:\n",
    "            self.layer5 = self._make_layer(block, layer_widths[5], layers[4], width=convsize, stride=2)\n",
    "            self.layer6 = self._make_layer(block, layer_widths[6], layers[5], width=convsize, stride=2)\n",
    "        else:\n",
    "            self.layer5 = None\n",
    "            self.layer6 = None\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, width=9, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, width=width, stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, width=width, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        if self.layer5 is not None:\n",
    "            x = self.layer5(x)\n",
    "            x = self.layer6(x)\n",
    "        x = x.squeeze(2)\n",
    "        return x\n",
    "\n",
    "def load_DAVEnet(v2=True):\n",
    "    if v2:\n",
    "        audio_model = ResDavenet(feat_dim=40, layers=[2,2,1,1,1,1], convsize=9,\n",
    "                                 layer_widths=[128,128,256,512,1024,2048,4096])\n",
    "    else:\n",
    "        audio_model = ResDavenet(feat_dim=40, layers=[2, 2, 2, 2], convsize=9,\n",
    "                                 layer_widths=[128, 128, 256, 512, 1024])\n",
    "\n",
    "    return audio_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context_Gating(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super(Context_Gating, self).__init__()\n",
    "        self.fc = nn.Linear(dimension, dimension)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fc(x)          \n",
    "        x = torch.cat((x, x1), 1)   # 차원 = 2 * dimension\n",
    "        return F.glu(x, 1)       # 차원 = dimension , glu가 반만 이용\n",
    "    \n",
    "class Gated_Embedding_Unit(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        super(Gated_Embedding_Unit, self).__init__()\n",
    "        self.fc = nn.Linear(input_dimension, output_dimension)  # 차원 맞추기\n",
    "        self.cg = Context_Gating(output_dimension)              # Context Gating \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)         \n",
    "        x = self.cg(x)         \n",
    "        return x               \n",
    "    \n",
    "class projection_net(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim=1024,\n",
    "            video_dim=4096,\n",
    "            we_dim=300,\n",
    "            cross_attention=False\n",
    "    ):\n",
    "        super(projection_net, self).__init__()\n",
    "        self.cross_attention = cross_attention\n",
    "\n",
    "    # Fuse적용 X\n",
    "        self.DAVEnet = load_DAVEnet(v2=True)\n",
    "        self.GU_audio = Gated_Embedding_Unit(4096, embed_dim)\n",
    "        self.GU_video = Gated_Embedding_Unit(video_dim, embed_dim)\n",
    "        self.GU_text_captions = Gated_Embedding_Unit(we_dim, embed_dim)\n",
    "\n",
    "    def forward(self, video, audio_input, nframes, text=None):\n",
    "        audio = self.DAVEnet(audio_input) # [16, 1024, 320]\n",
    "        audio = audio.permute(0,2,1)\n",
    "\n",
    "        # text = self.GU_text_captions(self.text_pooling_caption(text)) # [16,30,300] -> [16,4096]\n",
    "        text = self.GU_text_captions(text)\n",
    "        audio = self.GU_audio(audio) \n",
    "        video = self.GU_video(video) \n",
    "        return audio, text, video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., init_values=None,\n",
    "            drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, use_softmax=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.cross_attn = MultiHeadCrossAttention(d_model=dim, n_head=num_heads, use_softmax=use_softmax)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, k, q, attention_mask=None):\n",
    "        output = q + self.drop_path(self.cross_attn(self.norm1(k), self.norm1(q)))#, attention_mask))  ##### 1) query만 residual\n",
    "        output = output + self.drop_path(self.mlp(self.norm2(output)))\n",
    "        return output\n",
    "\n",
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self, use_softmax):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.use_softmax = use_softmax\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, e=1e-12):\n",
    "        # input size: [batch_size, head, length, d_tensor]\n",
    "        batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "        k_t = k.transpose(2,3)\n",
    "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "        if self.use_softmax:\n",
    "            score = self.softmax(score)  #[0,1]\n",
    "        v = score @ v\n",
    "\n",
    "        return v, score \n",
    "    \n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, use_softmax):\n",
    "        super(MultiHeadCrossAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.use_softmax = use_softmax\n",
    "        self.attention = ScaleDotProductAttention(use_softmax)\n",
    "\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, k, q):\n",
    "        v = k\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "        out, attention = self.attention(q, k, v)\n",
    "        out = self.concat(out)\n",
    "        out = self.w_concat(out)\n",
    "        return out \n",
    "\n",
    "    def split(self, tensor):\n",
    "        # [batch_size, length, d_model] -> [batch_size, head, length, d_model]\n",
    "        batch_size, length, d_model = tensor.size()\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1,2)\n",
    "        return tensor \n",
    "    \n",
    "    def concat(self, tensor):\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        tensor = tensor.transpose(1,2).contiguous().view(batch_size, length, self.d_model)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, depth=1, num_heads=64, mlp_ratio=1, qkv_bias=True,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=None,\n",
    "                 act_layer=None,\n",
    "                 use_cls_token=True,\n",
    "                 num_classes=20,\n",
    "                 use_softmax=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if use_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "\n",
    "        self.masking_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            FusionBlock(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer, use_softmax=use_softmax\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "\n",
    "        self.norm = norm_layer(embed_dim) # TODO: not needed, remove?\n",
    "\n",
    "        self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, key, query, key_modal='', query_modal=''):\n",
    "        token_k = key\n",
    "        token_q = query\n",
    "\n",
    "        # FusionBlock (cross attnetion)\n",
    "        for block in self.blocks:\n",
    "            tokens = block(token_k, token_q)\n",
    "        output = tokens\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer_1 = nn.Linear(latent_dim, 256)\n",
    "        self.layer_2 = nn.Linear(256, 128)\n",
    "        self.layer_3 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EverythingAtOnceModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 embed_dim=1024,\n",
    "                 video_embed_dim=4096,\n",
    "                 text_embed_dim=300,\n",
    "                 video_max_tokens=None,\n",
    "                 text_max_tokens=None,\n",
    "                 audio_max_num_STFT_frames=None,\n",
    "                 projection_dim=6144,\n",
    "                 projection='gated',\n",
    "                 strategy_audio_pooling='none',\n",
    "                 davenet_v2=True,\n",
    "                 individual_projections=True,\n",
    "                 use_positional_emb=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.use_softmax = True\n",
    "        self.use_cls_token = False\n",
    "        self.num_classes = 20\n",
    "\n",
    "        self.fusion = FusionTransformer(embed_dim=self.embed_dim, use_softmax=self.use_softmax, use_cls_token=self.use_cls_token, num_classes = self.num_classes)\n",
    "\n",
    "        self.token_projection = 'projection_net'\n",
    "\n",
    "        self.individual_projections = individual_projections\n",
    "        self.use_positional_emb = use_positional_emb\n",
    "        self.strategy_audio_pooling = strategy_audio_pooling\n",
    "\n",
    "        self.video_norm_layer = nn.LayerNorm(self.embed_dim, eps=1e-6)\n",
    "        self.text_norm_layer = nn.LayerNorm(self.embed_dim, eps=1e-6)\n",
    "        self.audio_norm_layer = nn.LayerNorm(self.embed_dim, eps=1e-6)\n",
    "        self.norm_layer = nn.LayerNorm(self.embed_dim, eps=1e-6)\n",
    "\n",
    "        # audio token preprocess\n",
    "        # self.davenet = load_DAVEnet(v2=davenet_v2)\n",
    "        self.token_proj = projection_net(embed_dim=self.embed_dim)\n",
    "\n",
    "        # self.commonencoder=CommonEncoder(common_dim=self.embed_dim, latent_dim=512)\n",
    "        self.classifier1 = Classifier(latent_dim=self.embed_dim, num_classes=self.num_classes)\n",
    "        self.classifier2 = Classifier(latent_dim=self.embed_dim, num_classes=self.num_classes)\n",
    "        self.classifier3 = Classifier(latent_dim=self.embed_dim, num_classes=self.num_classes)\n",
    "    \n",
    "    def extract_tokens(self, video, audio, text, nframes):\n",
    "        audio, text, video = self.token_proj(video, audio, nframes, text)\n",
    "        return audio, text, video\n",
    "\n",
    "    def forward(self, video, audio, nframes, text, category, force_cross_modal=False):\n",
    "        audio_raw_embed, text_raw_embed, video_raw_embed = self.extract_tokens(video, audio, text, nframes)\n",
    "        video_raw_embed = torch.unsqueeze(video_raw_embed, 1) # ([16, 1, 1024] [16, 80, 1024] [16, 30, 1024]\n",
    "\n",
    "\n",
    "        ## Visual - Audio\n",
    "        va = self.fusion(key=video_raw_embed, query=audio_raw_embed) # [16, 80, 20]\n",
    "        va = self.classifier1(va.mean(dim=1)) \n",
    "\n",
    "        ## Audio - Text\n",
    "        at = self.fusion(key=audio_raw_embed, query=text_raw_embed) # [16, 30, 20]\n",
    "        at = self.classifier2(at.mean(dim=1))\n",
    "\n",
    "        ## Text - Video\n",
    "        tv = self.fusion(key=text_raw_embed, query=video_raw_embed) # [16, 1, 20]\n",
    "        tv = self.classifier3(tv.mean(dim=1))\n",
    "\n",
    "        return va ,at, tv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'C:/Users/heeryung/code/24w_deep_daiv/ckpt/trial10/epoch120.pth'\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "model = EverythingAtOnceModel().cuda()\n",
    "model.load_state_dic(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "video = data['video']#.cuda()\n",
    "audio = data['audio']#.cuda()\n",
    "text = data['text']#.cuda()\n",
    "nframes = data['nframes']#.cuda()\n",
    "category = data['category']#.cuda()\n",
    "\n",
    "video = video.view(-1, video.shape[-1])\n",
    "audio = audio.view(-1, audio.shape[-2], audio.shape[-1])\n",
    "text = text.view(-1, text.shape[-2], text.shape[-1])\n",
    "\n",
    "pred = model(video, audio, nframes, text, category)\n",
    "pred_category = torch.argmax(pred, dim=1) \n",
    "# accuracy = torch.mean((pred_category == category).float()) \n",
    "print(\"Real category:\", category, 'Pred category:', pred_category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiwon2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
